{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10d3b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/28 19:01:21 WARN Utils: Your hostname, korto, resolves to a loopback address: 127.0.1.1; using 192.168.1.147 instead (on interface wlp4s0)\n",
      "25/07/28 19:01:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/data1/home/fidok/workspace_python/spark-lakehouse/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/fidok/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/fidok/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-08f7e9fd-dbea-4939-a71d-de27e6e59055;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 125ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-08f7e9fd-dbea-4939-a71d-de27e6e59055\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "25/07/28 19:01:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/28 19:01:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "\n",
    "# configure spark with Delta extension using Spark config options\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"helloWorld\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864eb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 17:57:22 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# create table using dataframe\n",
    "data = spark.range(0, 5)\n",
    "data.write.format(\"delta\").option(\"mode\", \"override\").save(\"data/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06415ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  4|\n",
      "|  2|\n",
      "|  3|\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read existing table as dataframe\n",
    "df = spark.read.format(\"delta\").load(\"data/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3ff067c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                       |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|0      |2025-07-28 17:57:21.835|NULL  |NULL    |WRITE    |{mode -> ErrorIfExists, partitionBy -> []}|NULL|NULL    |NULL     |NULL       |Serializable  |true         |{numFiles -> 6, numOutputRows -> 5, numOutputBytes -> 2766}|NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "+-------+-----------------------+------+--------+---------+------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "\n",
      "+------+------------------------------------+----+-----------+---------------------------------------------------------------------------------------------+-----------------------+-----------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|format|id                                  |name|description|location                                                                                     |createdAt              |lastModified           |partitionColumns|clusteringColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|tableFeatures           |\n",
      "+------+------------------------------------+----+-----------+---------------------------------------------------------------------------------------------+-----------------------+-----------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|delta |42bdfb60-54e4-41cc-8e77-76957de163f4|NULL|NULL       |file:/data1/home/fidok/workspace_python/spark-lakehouse/chapter_1/spark-conf/data/delta-table|2025-07-28 17:57:20.006|2025-07-28 17:57:21.835|[]              |[]               |5       |2470       |{}        |1               |2               |[appendOnly, invariants]|\n",
      "+------+------------------------------------+----+-----------+---------------------------------------------------------------------------------------------+-----------------------+-----------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read delta as DeltaTable type\n",
    "delta_table = DeltaTable.forPath(spark, \"data/delta-table\")\n",
    "delta_table.history().show(truncate=False)\n",
    "delta_table.detail().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9cbf858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table using schema and DeltaTableBuilder\n",
    "# Create or replace table with path and add properties\n",
    "st = DeltaTable.createOrReplace(spark) \\\n",
    "  .addColumn(\"id\", \"INT\") \\\n",
    "  .addColumn(\"firstName\", \"STRING\") \\\n",
    "  .addColumn(\"lastName\", \"STRING\", comment = \"surname\") \\\n",
    "  .addColumn(\"salary\", \"INT\") \\\n",
    "  .property(\"description\", \"table with people's salary\") \\\n",
    "  .location(\"/data1/home/fidok/workspace_python/spark-lakehouse/chapter_1/data/salary_table\") \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab418ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------+\n",
      "| id|firstName|lastName|salary|\n",
      "+---+---------+--------+------+\n",
      "+---+---------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "st.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae1e35f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------+\n",
      "| id|firstName|lastName|salary|\n",
      "+---+---------+--------+------+\n",
      "|  1|     John|    Does|  1500|\n",
      "|  2| LongDark|     Man|  2000|\n",
      "+---+---------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create table using Spark schema\n",
    "import os \n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"id\", T.IntegerType()),\n",
    "    T.StructField(\"firstName\", T.StringType()),\n",
    "    T.StructField(\"lastName\", T.StringType()),\n",
    "    T.StructField(\"salary\", T.IntegerType())\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1, \"John\", \"Does\", 1500),\n",
    "    (2, \"LongDark\", \"Man\", 2000)\n",
    "    ], schema=schema)\n",
    "\n",
    "df.show()\n",
    "\n",
    "delta_path = os.path.join(os.getcwd(), \"chapter_1/data/salary_table\")\n",
    "\n",
    "df.write.format(\"delta\").mode(\"append\").save(\"data/salary_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddcb2598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------+\n",
      "| id|firstName|lastName|salary|\n",
      "+---+---------+--------+------+\n",
      "|  2|Invisible|     Man|  2000|\n",
      "|  1|     John|     Doe|  1500|\n",
      "+---+---------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_df = DeltaTable.forPath(spark, \"data/salary_table\").toDF()\n",
    "salary_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
